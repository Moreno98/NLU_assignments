{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IApNoCxVjLBr"
      },
      "source": [
        "# Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMWyZGh0beLA",
        "outputId": "e0f51dcf-ba3e-4d1f-b806-a798e6fa23f5"
      },
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install spacy-conll"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/d8/0361bbaf7a1ff56b44dca04dace54c82d63dad7475b7d25ea1baefafafb2/spacy-3.0.6-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 254kB/s \n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Collecting pathy>=0.3.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/87/5991d87be8ed60beb172b4062dbafef18b32fa559635a8e2b633c2974f85/pathy-0.5.2-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.0.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 33.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Collecting catalogue<2.1.0,>=2.0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/9c/10/dbc1203a4b1367c7b02fddf08cb2981d9aa3e688d398f587cea0ab9e3bec/catalogue-2.0.4-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Collecting thinc<8.1.0,>=8.0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/67/d4002a18e26bf29b17ab563ddb55232b445ab6a02f97bf17d1345ff34d3f/spacy_legacy-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Collecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 41.6MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=2075dd96f40957efe58c3851dff42a8573bb38f048324fee10b7cbc97837b2a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: typer, smart-open, pathy, catalogue, srsly, pydantic, thinc, spacy-legacy, spacy\n",
            "  Found existing installation: smart-open 5.0.0\n",
            "    Uninstalling smart-open-5.0.0:\n",
            "      Successfully uninstalled smart-open-5.0.0\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.4 pathy-0.5.2 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.5 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n",
            "2021-04-29 17:33:06.288732: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 309kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (56.0.0)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting spacy-conll\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/58/a666c5bf086d082ebd83954cadcb803e342e6ce4a8c3fd270820ce79341c/spacy_conll-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: spacy>=2.0 in /usr/local/lib/python3.7/dist-packages (from spacy-conll) (3.0.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from spacy-conll) (20.9)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (1.19.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (1.7.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (2.0.5)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (8.0.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (0.3.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (3.0.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (2.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (2.23.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (56.0.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (2.0.4)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (3.7.4.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (3.0.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (4.41.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.0->spacy-conll) (0.5.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->spacy-conll) (2.4.7)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy>=2.0->spacy-conll) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0->spacy-conll) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0->spacy-conll) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0->spacy-conll) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0->spacy-conll) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy>=2.0->spacy-conll) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=2.0->spacy-conll) (1.1.1)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=2.0->spacy-conll) (3.0.0)\n",
            "Installing collected packages: spacy-conll\n",
            "Successfully installed spacy-conll-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYT93VkUlMIY"
      },
      "source": [
        "# Download dataset and conll script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vb42FwUKjhOn",
        "outputId": "9047ec8b-fd44-4854-929d-6ae6bfc86900"
      },
      "source": [
        "!wget -O /content/conll2003.zip https://github.com/esrel/NLU.Lab.2021/blob/master/src/conll2003.zip?raw=true\n",
        "!wget https://raw.githubusercontent.com/esrel/NLU.Lab.2021/master/src/conll.py"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-29 17:33:18--  https://github.com/esrel/NLU.Lab.2021/blob/master/src/conll2003.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/esrel/NLU.Lab.2021/raw/master/src/conll2003.zip [following]\n",
            "--2021-04-29 17:33:18--  https://github.com/esrel/NLU.Lab.2021/raw/master/src/conll2003.zip\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/esrel/NLU.Lab.2021/master/src/conll2003.zip [following]\n",
            "--2021-04-29 17:33:18--  https://raw.githubusercontent.com/esrel/NLU.Lab.2021/master/src/conll2003.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 959816 (937K) [application/zip]\n",
            "Saving to: ‘/content/conll2003.zip’\n",
            "\n",
            "/content/conll2003. 100%[===================>] 937.32K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2021-04-29 17:33:18 (40.9 MB/s) - ‘/content/conll2003.zip’ saved [959816/959816]\n",
            "\n",
            "--2021-04-29 17:33:18--  https://raw.githubusercontent.com/esrel/NLU.Lab.2021/master/src/conll.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6148 (6.0K) [text/plain]\n",
            "Saving to: ‘conll.py’\n",
            "\n",
            "conll.py            100%[===================>]   6.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-04-29 17:33:19 (51.7 MB/s) - ‘conll.py’ saved [6148/6148]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIXGWBhvlXnz"
      },
      "source": [
        "Unzip the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPFh7B0e9vXG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b561b62-4ddf-4c0d-c309-3096b97e1c3b"
      },
      "source": [
        "!unzip conll2003.zip -d /content/dataset/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  conll2003.zip\n",
            "  inflating: /content/dataset/dev.txt  \n",
            "  inflating: /content/dataset/__MACOSX/._dev.txt  \n",
            "  inflating: /content/dataset/test.txt  \n",
            "  inflating: /content/dataset/__MACOSX/._test.txt  \n",
            "  inflating: /content/dataset/train.txt  \n",
            "  inflating: /content/dataset/__MACOSX/._train.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agKxCf-SjQBd"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxU4tgLMcYm_"
      },
      "source": [
        "import spacy, nltk\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "import pandas as pd\n",
        "import conll\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sccZl8AOl8Rd"
      },
      "source": [
        "# 1) Evaluate spaCy NER on CoNLL 2003 data (provided)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhCoTgwSmGAJ"
      },
      "source": [
        "import_dataset(path):  \n",
        "  * Input: the path of the dataset to load\n",
        "  * Output: two lists:\n",
        "    1. text_dataset: contains the lists of sentences of the dataset as text\n",
        "    2. dataset: contais the pair (token, name entity) for each token, divided in sentences (one list for each sentence)\n",
        "  * Implementation: it reads the dataset using conll function, for each sentence it extracts the tokens as text or the tuple (token, name entity) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyVRjyP4p2DE"
      },
      "source": [
        "def import_dataset(path):\n",
        "  data = conll.read_corpus_conll(path)\n",
        "  text_dataset = []\n",
        "  dataset = []\n",
        "  for t in data:\n",
        "    sentence = []\n",
        "    txt = \"\"\n",
        "    for t2 in t:\n",
        "      sentence.append((t2[0].split()[0], t2[0].split()[3]))\n",
        "      txt += str(t2[0].split()[0]) + \" \"\n",
        "    dataset.append(sentence)\n",
        "    text_dataset.append([txt])\n",
        "  return text_dataset, dataset"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4zj_Ymnu6kL"
      },
      "source": [
        "convert_type(ent_type):\n",
        "  * Input: named entity from spaCy\n",
        "  * Output: the name entity converted in the dataset format\n",
        "  * Implementation: assign a specific named entity from the dataset format to each named entity from spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54cpPUu4TQkv"
      },
      "source": [
        "def convert_type(ent_type):\n",
        "  if(ent_type in [\"ORG\", \"WORK_OF_ART\"]):\n",
        "    return \"ORG\" \n",
        "  if(ent_type in [\"GPE\", \"LOC\"]):\n",
        "    return \"LOC\"\n",
        "  if(ent_type in [\"LANGUAGE\", \"NORP\", \"EVENT\", \"LAW\", \"PRODUCT\", \"MONEY\"]):\n",
        "    return \"MISC\"\n",
        "  if(ent_type in [\"PERSON\", \"FAC\"]):\n",
        "    return \"PER\"\n",
        "  return \"\""
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN2SvB5WBN85"
      },
      "source": [
        "convert_spacy(token, parent=None, parent_iob=None):\n",
        "  * Input: \n",
        "    * Token: the token to convert\n",
        "    * Parent: the parent of the token to use to group the named entity\n",
        "    * Parent_iob: this tag is used to know if there is a parent in the tree which already has the B tag, if yes the I tag will be assigned to the token, B otherwise.\n",
        "  * Output: the tags converted in form ```iob-type``` as in the dataset\n",
        "  * Implementation: \n",
        "    * if parent is None it returns just the concatenation between the ```IOB``` tag and the named entity tag\n",
        "    * if parent is set it returns the named entity from the parent if possible\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZmRntim-jYH"
      },
      "source": [
        "def convert_spacy(token, parent=None, parent_iob=None): # parent_iob maybe different from the current parent.ent_iob_\n",
        "  if(parent == None): # exercise 1 usage\n",
        "    if(convert_type(token.ent_type_) == \"\"):\n",
        "      return \"O\"\n",
        "    else:\n",
        "      return f\"{token.ent_iob_}-{convert_type(token.ent_type_)}\"\n",
        "  else: # exercise 3 usage\n",
        "    iob = \"I\"\n",
        "    if(parent_iob == \"I\"):\n",
        "      iob = \"B\"\n",
        "    if(parent.ent_type_ != \"\"):\n",
        "      if(convert_type(parent.ent_type_) == \"\"):\n",
        "        return \"O\"\n",
        "      return f\"{iob}-{convert_type(parent.ent_type_)}\"\n",
        "    else:\n",
        "      if(token.ent_iob_ == \"O\"):\n",
        "        return \"O\"\n",
        "      else:\n",
        "        if(convert_type(token.ent_type_) == \"\"):\n",
        "          return \"O\"\n",
        "        return f\"{token.ent_iob_}-{convert_type(token.ent_type_)}\""
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIR0VkanE59W"
      },
      "source": [
        "reconstruct_output(doc, comp=False, ancestors = True):\n",
        "  * Input: \n",
        "    * Doc object from spaCy \n",
        "    * comp (compound) flag to set on the third exercise\n",
        "    * ancestors: this parameter is used in the final experiment of the third point, where I try just the direct parent of the token and not the entire tree\n",
        "  * Output: list of sentences, each sentence contains the token \"reconstructed\" as in the dataset\n",
        "  * Implementation: \n",
        "    * given a token it uses whitespace to check if the token is part of a word in the dataset, if yes it concatenates the tokens with the same tag, otherwise the single token is used.  \n",
        "    * if comp is set to True, the tokens with compound dependency will have the same tag as the first parent with a dependency different from \"compound\" (if possible) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zI3z9hJGP6j"
      },
      "source": [
        "def reconstruct_output(doc, comp=False, ancestors = True):\n",
        "  output = []\n",
        "  current_token = \"\"\n",
        "  current_tag = \"\"\n",
        "  first = True\n",
        "  for token in doc:\n",
        "    if(first):\n",
        "        current_tag = convert_spacy(token)\n",
        "        if((comp) and (token.dep_ == \"compound\")):\n",
        "          parent = token.head\n",
        "          parent_iob = token.head.ent_iob_\n",
        "          while((parent.dep_ == \"compound\") and (ancestors)):\n",
        "            if(parent_iob != \"B\"):\n",
        "              parent_iob = parent.head.ent_iob_\n",
        "            parent = parent.head\n",
        "          current_tag = convert_spacy(token, parent, parent_iob)\n",
        "        first = False\n",
        "    if(not token.whitespace_):\n",
        "      current_token += token.text\n",
        "    else:\n",
        "      current_token += token.text\n",
        "      output.append((current_token, current_tag))\n",
        "      first = True\n",
        "      current_token = \"\"\n",
        "      current_tag = \"\"\n",
        "  if(not first):\n",
        "    output.append((current_token, current_tag))\n",
        "  \n",
        "  return output"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Au1bFTj2KnXn"
      },
      "source": [
        "process_dataset(dataset_text, expand, ancestors):\n",
        "  * Input: the dataset as lists of sentences, expand is a flag used in the third exercise to expand the named entities as well as the ancestors\n",
        "  * Output: the predicted named entities\n",
        "  * Implementation: it processes each sentence using nlp and it calls reconstruct_output to format it as in the dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5DsAofjC0-S"
      },
      "source": [
        "def process_dataset(dataset_text, expand, ancestors):\n",
        "  pred = []\n",
        "  for sentence in dataset_text:\n",
        "    spacy_output = nlp(sentence[0])\n",
        "    pred.append(reconstruct_output(spacy_output, expand, ancestors))\n",
        "  return pred"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS5AnKvrM66e"
      },
      "source": [
        "get_accuracy(dataset_text, dataset_refs, expand = False, ancestors = True):\n",
        "  * Input: \n",
        "    * dataset_text: the dataset as lists of sentences (text)\n",
        "    * dataset_refs: the true named entities from the dataset\n",
        "    * expand: whether to use the expanded version (ex3) or not\n",
        "    * ancestors: usually True, just used in the final experiment (ex3)\n",
        "  * Output:\n",
        "    * the scikit classification report of spaCy NER on the specified dataset (using the setting on convert_type function)\n",
        "    * the predictions\n",
        "  * Implementation: process the dataset and compute the report\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFetBbV1M6eC"
      },
      "source": [
        "def get_accuracy(dataset_text, dataset_refs, expand = False, ancestors = True):\n",
        "  pred = process_dataset(dataset_text, expand, ancestors)\n",
        "  predicted = []\n",
        "\n",
        "  for sentence in pred:\n",
        "    for token in sentence:\n",
        "      predicted.append(token[1])\n",
        "  \n",
        "  true_labels = []\n",
        "  for sentence in dataset_refs:\n",
        "    for token in sentence:\n",
        "      true_labels.append(token[1])\n",
        "\n",
        "  report = classification_report(true_labels, predicted)\n",
        "\n",
        "  return report, pred"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcWWz-8YOlyy"
      },
      "source": [
        "# -) Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxKy3C_7kZyq"
      },
      "source": [
        "dev_path = '/content/dataset/dev.txt'\n",
        "train_path = '/content/dataset/train.txt'\n",
        "test_path = '/content/dataset/test.txt'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRdj6jB8PFSp"
      },
      "source": [
        "Extract the datasets as:\n",
        " * *_txt: list of sentences as text\n",
        " * *_refs: the true named entities from each dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzxO2PfjXire"
      },
      "source": [
        "dev_txt, dev_refs = import_dataset(dev_path)\n",
        "train_txt, train_refs = import_dataset(train_path)\n",
        "test_txt, test_refs = import_dataset(test_path)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1bFlrA7PVUp"
      },
      "source": [
        "1.1) Compute the token level accuracy for the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGod1DPJCOWQ",
        "outputId": "2e8a85cc-9458-483c-c3dc-5153bbbca3a2"
      },
      "source": [
        "report_test, pred = get_accuracy(test_txt, test_refs)\n",
        "print(report_test)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.77      0.68      0.72      1668\n",
            "      B-MISC       0.58      0.55      0.57       702\n",
            "       B-ORG       0.51      0.31      0.38      1661\n",
            "       B-PER       0.79      0.63      0.70      1617\n",
            "       I-LOC       0.57      0.53      0.55       257\n",
            "      I-MISC       0.26      0.36      0.30       216\n",
            "       I-ORG       0.41      0.52      0.46       835\n",
            "       I-PER       0.82      0.79      0.80      1156\n",
            "           O       0.95      0.97      0.96     38554\n",
            "\n",
            "    accuracy                           0.90     46666\n",
            "   macro avg       0.63      0.59      0.61     46666\n",
            "weighted avg       0.90      0.90      0.90     46666\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nkdxhz83Pavg"
      },
      "source": [
        "1.2) Compute the chunk level accuracy for the test set using the evaluate function provided by conll.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "6n2B6NQzI-ct",
        "outputId": "087a821e-5a82-4628-adfc-836b79ea5617"
      },
      "source": [
        "results = conll.evaluate(test_refs, pred)\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.760</td>\n",
              "      <td>0.610</td>\n",
              "      <td>0.677</td>\n",
              "      <td>1617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.460</td>\n",
              "      <td>0.277</td>\n",
              "      <td>0.346</td>\n",
              "      <td>1661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.755</td>\n",
              "      <td>0.667</td>\n",
              "      <td>0.708</td>\n",
              "      <td>1668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.576</td>\n",
              "      <td>0.546</td>\n",
              "      <td>0.560</td>\n",
              "      <td>702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.663</td>\n",
              "      <td>0.521</td>\n",
              "      <td>0.583</td>\n",
              "      <td>5648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f     s\n",
              "PER    0.760  0.610  0.677  1617\n",
              "ORG    0.460  0.277  0.346  1661\n",
              "LOC    0.755  0.667  0.708  1668\n",
              "MISC   0.576  0.546  0.560   702\n",
              "total  0.663  0.521  0.583  5648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4BRlFotT4DK"
      },
      "source": [
        "# -) Experiment\n",
        "Here I was curious about using already tokenized text from the dataset (overriding spaCy tokenizer).  \n",
        "Despite spaCy's documentation reports that the performance should decrease (due to the fact that the tokenization methods may be different) the perfomance remains similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLyH6Jc3wSbB"
      },
      "source": [
        "from spacy.tokens import Doc\n",
        "\n",
        "# function to replace spaCy tokenizer\n",
        "def get_tokens(sentence):\n",
        "  return Doc(nlp.vocab, sentence)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkAsnqLrn6y7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4ec7d78-eb1e-4cdb-d4ac-1200f7492f11"
      },
      "source": [
        "nlp.tokenizer = get_tokens\n",
        "\n",
        "data = conll.read_corpus_conll(test_path)\n",
        "pred = []\n",
        "\n",
        "for s in data:\n",
        "  sentence = []\n",
        "  for token in s:\n",
        "    sentence.append(token[0].split()[0])\n",
        "  doc = nlp(sentence)\n",
        "  pred.append(reconstruct_output(doc))\n",
        "\n",
        "predicted = []\n",
        "for sentence in pred:\n",
        "  for token in sentence:\n",
        "    predicted.append(token[1])\n",
        "\n",
        "true_labels = []\n",
        "for sentence in test_refs:\n",
        "  for token in sentence:\n",
        "    true_labels.append(token[1])\n",
        "\n",
        "report = classification_report(true_labels, predicted)\n",
        "print(report)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.78      0.70      0.74      1668\n",
            "      B-MISC       0.58      0.55      0.56       702\n",
            "       B-ORG       0.50      0.30      0.38      1661\n",
            "       B-PER       0.77      0.61      0.68      1617\n",
            "       I-LOC       0.60      0.62      0.61       257\n",
            "      I-MISC       0.27      0.37      0.31       216\n",
            "       I-ORG       0.41      0.52      0.46       835\n",
            "       I-PER       0.80      0.76      0.78      1156\n",
            "           O       0.95      0.97      0.96     38554\n",
            "\n",
            "    accuracy                           0.90     46666\n",
            "   macro avg       0.63      0.60      0.61     46666\n",
            "weighted avg       0.90      0.90      0.90     46666\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "OoLtS0QoT0uG",
        "outputId": "d99eefff-ff86-46dd-a407-03167f35e525"
      },
      "source": [
        "results = conll.evaluate(test_refs, pred)\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.748</td>\n",
              "      <td>0.592</td>\n",
              "      <td>0.661</td>\n",
              "      <td>1617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.444</td>\n",
              "      <td>0.273</td>\n",
              "      <td>0.338</td>\n",
              "      <td>1661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.766</td>\n",
              "      <td>0.695</td>\n",
              "      <td>0.729</td>\n",
              "      <td>1668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.571</td>\n",
              "      <td>0.541</td>\n",
              "      <td>0.556</td>\n",
              "      <td>702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.659</td>\n",
              "      <td>0.522</td>\n",
              "      <td>0.583</td>\n",
              "      <td>5648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f     s\n",
              "PER    0.748  0.592  0.661  1617\n",
              "ORG    0.444  0.273  0.338  1661\n",
              "LOC    0.766  0.695  0.729  1668\n",
              "MISC   0.571  0.541  0.556   702\n",
              "total  0.659  0.522  0.583  5648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXeh7MMaBE3Y"
      },
      "source": [
        "# 2) Grouping of Entities.  \n",
        "Write a function to group recognized named entities using noun_chunks method of spaCy. Analyze the groups in terms of most frequent combinations (i.e. NER types that go together)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRhPjDtuB7T0"
      },
      "source": [
        "group_eintities(sentence):\n",
        "  * Input: the sentence to process\n",
        "  * Output: named entities grouped based on noun_chunk\n",
        "  * Implementation:\n",
        "    * first a set containing all the sentence entities is created\n",
        "    * for each noun_chunk its entities are checked if they belong to the main entity set, if yes they will be part of the chunk group\n",
        "    * the entities added are removed from the main set\n",
        "    * in the end if the set is not empty, each remaining entity is added to a different new chunk (entities that were not in any chunk)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrgupFfQNKnf"
      },
      "source": [
        "# I checked whether all the entities of the sentence (doc.ents) are inside chunk.ents.\n",
        "# there might be new entities inside chunk.ents, they will be discarded, so just the main entities from the sentence will be considered.\n",
        "\n",
        "def group_entities(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  groups = []\n",
        "  entities = set()\n",
        "\n",
        "  for ent in doc.ents:\n",
        "    entities.add(ent)\n",
        "\n",
        "  for chunk in doc.noun_chunks:\n",
        "    group = []\n",
        "    for span in chunk.ents:\n",
        "      if span in entities:\n",
        "        group.append(span.root.ent_type_)\n",
        "        entities.remove(span)\n",
        "    if(len(group) != 0):\n",
        "      groups.append(group)\n",
        "\n",
        "  for ent in entities:\n",
        "    groups.append([ent.root.ent_type_])\n",
        "\n",
        "  return groups"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBqHog1tIDup"
      },
      "source": [
        "get_frequencies(dataset):\n",
        "  * Input: the dataset where counting the combinations of entities\n",
        "  * Output: a dict containing the frequencies for each combination\n",
        "  * Implementation:\n",
        "    * process each sentence of the dataset and groups its entities using group_entities\n",
        "    * for each group create a tuple and increase the count of that group (combination) on the dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Njvl8hVdzdA"
      },
      "source": [
        "def get_frequencies(dataset):\n",
        "  freq = dict()\n",
        "  for sentence in dataset:\n",
        "    groups = group_entities(sentence[0])\n",
        "    for group in groups:\n",
        "      group = tuple(group)\n",
        "      if(group in freq):\n",
        "        freq[group] += 1\n",
        "      else:\n",
        "        freq[group] = 1\n",
        "  return freq"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iGHVxW5JM4t"
      },
      "source": [
        "**Get the frequencies of the test set**  \n",
        "Print the dictionary containing the frequencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW2tiJHKeStO",
        "outputId": "cb7c4abd-2359-4020-d8dc-5b861d151aef"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm') # reset the tokenizer if 1.experiment has been run\n",
        "freq = get_frequencies(test_txt)\n",
        "freq_ordered = dict(sorted(freq.items(), key=lambda item: item[1], reverse=True))\n",
        "print(\"Frequencies:\")\n",
        "for group in freq_ordered:\n",
        "  print(f\"{group} -> {freq_ordered[group]}\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frequencies:\n",
            "('CARDINAL',) -> 1624\n",
            "('GPE',) -> 1255\n",
            "('PERSON',) -> 1074\n",
            "('DATE',) -> 997\n",
            "('ORG',) -> 873\n",
            "('NORP',) -> 293\n",
            "('MONEY',) -> 147\n",
            "('ORDINAL',) -> 111\n",
            "('TIME',) -> 83\n",
            "('PERCENT',) -> 81\n",
            "('EVENT',) -> 58\n",
            "('LOC',) -> 54\n",
            "('CARDINAL', 'PERSON') -> 51\n",
            "('QUANTITY',) -> 51\n",
            "('NORP', 'PERSON') -> 43\n",
            "('GPE', 'PERSON') -> 34\n",
            "('GPE', 'GPE') -> 26\n",
            "('FAC',) -> 22\n",
            "('PRODUCT',) -> 22\n",
            "('ORG', 'PERSON') -> 21\n",
            "('CARDINAL', 'ORG') -> 19\n",
            "('CARDINAL', 'NORP') -> 15\n",
            "('CARDINAL', 'GPE') -> 13\n",
            "('GPE', 'ORG') -> 13\n",
            "('LAW',) -> 11\n",
            "('WORK_OF_ART',) -> 10\n",
            "('GPE', 'PRODUCT') -> 9\n",
            "('DATE', 'EVENT') -> 8\n",
            "('DATE', 'ORG') -> 8\n",
            "('ORG', 'ORG') -> 8\n",
            "('PERSON', 'PERSON') -> 8\n",
            "('NORP', 'ORG') -> 8\n",
            "('DATE', 'TIME') -> 7\n",
            "('ORG', 'DATE') -> 6\n",
            "('LANGUAGE',) -> 6\n",
            "('GPE', 'DATE') -> 5\n",
            "('CARDINAL', 'CARDINAL') -> 5\n",
            "('NORP', 'ORDINAL') -> 5\n",
            "('ORG', 'GPE') -> 5\n",
            "('DATE', 'NORP') -> 5\n",
            "('GPE', 'ORDINAL') -> 4\n",
            "('ORDINAL', 'PERSON') -> 4\n",
            "('GPE', 'CARDINAL') -> 4\n",
            "('ORG', 'NORP') -> 4\n",
            "('PERSON', 'GPE') -> 4\n",
            "('CARDINAL', 'DATE') -> 3\n",
            "('ORG', 'CARDINAL') -> 3\n",
            "('CARDINAL', 'PERSON', 'CARDINAL') -> 3\n",
            "('NORP', 'NORP') -> 3\n",
            "('PERSON', 'PERSON', 'PERSON') -> 2\n",
            "('CARDINAL', 'ORDINAL') -> 2\n",
            "('CARDINAL', 'CARDINAL', 'PERSON') -> 2\n",
            "('ORG', 'ORDINAL') -> 2\n",
            "('LANGUAGE', 'ORDINAL') -> 2\n",
            "('GPE', 'DATE', 'ORG') -> 2\n",
            "('ORDINAL', 'EVENT') -> 2\n",
            "('GPE', 'LOC') -> 2\n",
            "('CARDINAL', 'CARDINAL', 'ORG') -> 2\n",
            "('QUANTITY', 'QUANTITY') -> 2\n",
            "('GPE', 'NORP') -> 2\n",
            "('DATE', 'CARDINAL') -> 2\n",
            "('DATE', 'NORP', 'PERSON') -> 2\n",
            "('EVENT', 'CARDINAL') -> 2\n",
            "('GPE', 'FAC') -> 2\n",
            "('PERSON', 'CARDINAL') -> 2\n",
            "('ORDINAL', 'NORP') -> 1\n",
            "('GPE', 'PERSON', 'CARDINAL') -> 1\n",
            "('ORDINAL', 'DATE') -> 1\n",
            "('ORG', 'GPE', 'ORDINAL') -> 1\n",
            "('ORG', 'QUANTITY') -> 1\n",
            "('CARDINAL', 'GPE', 'GPE') -> 1\n",
            "('PERCENT', 'CARDINAL') -> 1\n",
            "('NORP', 'DATE') -> 1\n",
            "('PERSON', 'NORP') -> 1\n",
            "('PERSON', 'ORG') -> 1\n",
            "('LOC', 'DATE') -> 1\n",
            "('DATE', 'FAC') -> 1\n",
            "('CARDINAL', 'CARDINAL', 'NORP') -> 1\n",
            "('NORP', 'PERSON', 'DATE') -> 1\n",
            "('LOC', 'ORDINAL') -> 1\n",
            "('PRODUCT', 'GPE') -> 1\n",
            "('MONEY', 'ORG') -> 1\n",
            "('NORP', 'LOC') -> 1\n",
            "('ORDINAL', 'GPE') -> 1\n",
            "('MONEY', 'DATE') -> 1\n",
            "('CARDINAL', 'PERCENT') -> 1\n",
            "('DATE', 'WORK_OF_ART') -> 1\n",
            "('MONEY', 'MONEY') -> 1\n",
            "('MONEY', 'CARDINAL', 'CARDINAL', 'ORG') -> 1\n",
            "('CARDINAL', 'GPE', 'TIME') -> 1\n",
            "('FAC', 'GPE') -> 1\n",
            "('ORG', 'WORK_OF_ART') -> 1\n",
            "('GPE', 'ORDINAL', 'PERSON') -> 1\n",
            "('CARDINAL', 'LOC') -> 1\n",
            "('PERSON', 'MONEY') -> 1\n",
            "('MONEY', 'PRODUCT') -> 1\n",
            "('DATE', 'PERSON') -> 1\n",
            "('PERSON', 'GPE', 'CARDINAL') -> 1\n",
            "('ORDINAL', 'ORG') -> 1\n",
            "('PERSON', 'ORDINAL') -> 1\n",
            "('EVENT', 'ORDINAL') -> 1\n",
            "('PERSON', 'FAC') -> 1\n",
            "('ORDINAL', 'TIME') -> 1\n",
            "('DATE', 'LANGUAGE', 'ORDINAL') -> 1\n",
            "('CARDINAL', 'EVENT') -> 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km-d8J_lBbRM"
      },
      "source": [
        "# 3) One of the possible post-processing steps is to fix segmentation errors.  \n",
        "Write a function that extends the entity span to cover the full noun-compounds. Make use of compound dependency relation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVA27TYsKS14"
      },
      "source": [
        "For this point I reused the get_accuracy function of the first point.  \n",
        "In this case the expand flag is set to True, this means that to the tokens with compound dependence will be assigned the tag of their parents (if possible)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_3ErKMHIfbu",
        "outputId": "aed31680-4479-40ad-f99b-25f9292eac4d"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm') # reset the tokenizer if 1.experiment has been run\n",
        "report_test, pred = get_accuracy(test_txt, test_refs, expand=True)\n",
        "print(report_test)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.77      0.67      0.72      1668\n",
            "      B-MISC       0.57      0.57      0.57       702\n",
            "       B-ORG       0.43      0.34      0.38      1661\n",
            "       B-PER       0.62      0.64      0.63      1617\n",
            "       I-LOC       0.46      0.49      0.48       257\n",
            "      I-MISC       0.29      0.35      0.32       216\n",
            "       I-ORG       0.42      0.39      0.40       835\n",
            "       I-PER       0.81      0.74      0.77      1156\n",
            "           O       0.95      0.97      0.96     38554\n",
            "\n",
            "    accuracy                           0.89     46666\n",
            "   macro avg       0.59      0.57      0.58     46666\n",
            "weighted avg       0.89      0.89      0.89     46666\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "bd9dA3eHdICv",
        "outputId": "4a7b5e3a-3416-4e59-b8cd-7d9e306d3be8"
      },
      "source": [
        "results = conll.evaluate(test_refs, pred)\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.549</td>\n",
              "      <td>0.581</td>\n",
              "      <td>0.565</td>\n",
              "      <td>1617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.332</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.295</td>\n",
              "      <td>1661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.733</td>\n",
              "      <td>0.659</td>\n",
              "      <td>0.694</td>\n",
              "      <td>1668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.554</td>\n",
              "      <td>0.560</td>\n",
              "      <td>0.557</td>\n",
              "      <td>702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.548</td>\n",
              "      <td>0.509</td>\n",
              "      <td>0.527</td>\n",
              "      <td>5648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f     s\n",
              "PER    0.549  0.581  0.565  1617\n",
              "ORG    0.332  0.265  0.295  1661\n",
              "LOC    0.733  0.659  0.694  1668\n",
              "MISC   0.554  0.560  0.557   702\n",
              "total  0.548  0.509  0.527  5648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvS-ijwTN615"
      },
      "source": [
        "As we can see, using this method, the performance slightly decreases.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJRToe0qFXn-"
      },
      "source": [
        "As final experiment, for curiosity, I try to use just the direct parent of the node and not the whole ancestors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AJ4aACfGC3V",
        "outputId": "28be6616-e3af-40d2-d879-8fb35ccea438"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm') # reset the tokenizer if 1.experiment has been run\n",
        "report_test, pred = get_accuracy(test_txt, test_refs, expand=True, ancestors = False)\n",
        "print(report_test)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.77      0.68      0.72      1668\n",
            "      B-MISC       0.57      0.58      0.57       702\n",
            "       B-ORG       0.43      0.34      0.38      1661\n",
            "       B-PER       0.66      0.64      0.65      1617\n",
            "       I-LOC       0.48      0.50      0.49       257\n",
            "      I-MISC       0.29      0.34      0.31       216\n",
            "       I-ORG       0.44      0.39      0.42       835\n",
            "       I-PER       0.81      0.74      0.78      1156\n",
            "           O       0.95      0.97      0.96     38554\n",
            "\n",
            "    accuracy                           0.90     46666\n",
            "   macro avg       0.60      0.58      0.59     46666\n",
            "weighted avg       0.89      0.90      0.89     46666\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "HVa7odMtGGJM",
        "outputId": "b358b7b8-fc63-4256-e4ef-3e1d71364830"
      },
      "source": [
        "results = conll.evaluate(test_refs, pred)\n",
        "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
        "pd_tbl.round(decimals=3)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>p</th>\n",
              "      <th>r</th>\n",
              "      <th>f</th>\n",
              "      <th>s</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PER</th>\n",
              "      <td>0.587</td>\n",
              "      <td>0.583</td>\n",
              "      <td>0.585</td>\n",
              "      <td>1617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ORG</th>\n",
              "      <td>0.330</td>\n",
              "      <td>0.272</td>\n",
              "      <td>0.298</td>\n",
              "      <td>1661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LOC</th>\n",
              "      <td>0.732</td>\n",
              "      <td>0.668</td>\n",
              "      <td>0.699</td>\n",
              "      <td>1668</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MISC</th>\n",
              "      <td>0.552</td>\n",
              "      <td>0.570</td>\n",
              "      <td>0.561</td>\n",
              "      <td>702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>total</th>\n",
              "      <td>0.557</td>\n",
              "      <td>0.515</td>\n",
              "      <td>0.535</td>\n",
              "      <td>5648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           p      r      f     s\n",
              "PER    0.587  0.583  0.585  1617\n",
              "ORG    0.330  0.272  0.298  1661\n",
              "LOC    0.732  0.668  0.699  1668\n",
              "MISC   0.552  0.570  0.561   702\n",
              "total  0.557  0.515  0.535  5648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFams2gedeZD"
      },
      "source": [
        "As we can see the perfomance are similar with a slightly increase at chunk level, however this method does not have much sense since the IOB tag and named entity tag will be chosen without considering the whole span."
      ]
    }
  ]
}