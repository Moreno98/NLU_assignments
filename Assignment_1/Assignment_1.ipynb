{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbyjVS03yld-"
      },
      "source": [
        "**This is the code for the first assignment of the NLU course**  \n",
        "Here are some basic explanations of the workflow of the functions, please refer to the report (README.md) for further explanations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmFpOM7i70Ob"
      },
      "source": [
        "# Install spaCy and download the trained pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfNrRTVCCSpO",
        "outputId": "e7f53ba5-4582-4a93-daf4-c5d74e136aee"
      },
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spacy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/70/a0b8bd0cb54d8739ba4d6fb3458785c3b9b812b7fbe93b0f10beb1a53ada/spacy-3.0.5-cp37-cp37m-manylinux2014_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 290kB/s \n",
            "\u001b[?25hCollecting pydantic<1.8.0,>=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b3/0a/52ae1c659fc08f13dd7c0ae07b88e4f807ad83fb9954a59b0b0a3d1a8ab6/pydantic-1.7.3-cp37-cp37m-manylinux2014_x86_64.whl (9.1MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1MB 39.6MB/s \n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/90/34/d138832f6945432c638f32137e6c79a3b682f06a63c488dcfaca6b166c64/typer-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting srsly<3.0.0,>=2.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/84/dfdfc9f6f04f6b88207d96d9520b911e5fec0c67ff47a0dea31ab5429a1e/srsly-2.4.1-cp37-cp37m-manylinux2014_x86_64.whl (456kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.10.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/78/d8/e25bc7f99877de34def57d36769f0cce4e895b374cdc766718efc724f9ac/spacy_legacy-3.0.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (54.2.0)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/53/97dc0197cca9357369b3b71bf300896cf2d3604fa60ffaaf5cbc277de7de/pathy-0.4.0-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Collecting thinc<8.1.0,>=8.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/87/decceba68a0c6ca356ddcb6aea8b2500e71d9bc187f148aae19b747b7d3c/thinc-8.0.3-cp37-cp37m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Collecting catalogue<2.1.0,>=2.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/82/a5/b5021c74c04cac35a27d34cbf3146d86eb8e173b4491888bc4908c4c8b3b/catalogue-2.0.3-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
            "Collecting smart-open<4.0.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/9a/ba2d5f67f25e8d5bbf2fcec7a99b1e38428e83cb715f64dd179ca43a11bb/smart_open-3.0.0.tar.gz (113kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 30.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Building wheels for collected packages: smart-open\n",
            "  Building wheel for smart-open (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for smart-open: filename=smart_open-3.0.0-cp37-none-any.whl size=107098 sha256=42a3a662bc6f1e8cf039d869ec1271eeba9634175372a29a817e44832f93fc13\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/88/7c/f06dabd5e9cabe02d2269167bcacbbf9b47d0c0ff7d6ebcb78\n",
            "Successfully built smart-open\n",
            "Installing collected packages: pydantic, typer, catalogue, srsly, spacy-legacy, smart-open, pathy, thinc, spacy\n",
            "  Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Found existing installation: smart-open 5.0.0\n",
            "    Uninstalling smart-open-5.0.0:\n",
            "      Successfully uninstalled smart-open-5.0.0\n",
            "  Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "Successfully installed catalogue-2.0.3 pathy-0.4.0 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.5 spacy-legacy-3.0.2 srsly-2.4.1 thinc-8.0.3 typer-0.3.2\n",
            "2021-04-20 13:07:25.000436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Collecting en-core-web-sm==3.0.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 322kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.3)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.10.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (54.2.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.1->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.0.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_wQOkCpBL0e"
      },
      "source": [
        "# Import spaCy and load the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCPFQWpYCXlj"
      },
      "source": [
        "import spacy \n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ame8urTxBb9p"
      },
      "source": [
        "# A simple function which parses the input sentence\n",
        "\n",
        "This function will be used in multiple points of the assignment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1KQ_ivEBtVg"
      },
      "source": [
        "def process_sentence(sentence):\n",
        "  return nlp(sentence)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19JqgfZ4Fe0r"
      },
      "source": [
        "# 1) Extract a path of dependency relations from the ROOT to a token\n",
        "\n",
        "\n",
        "*   path_to_root(sentence)\n",
        "    * Input: the sentence to parse\n",
        "    * Output: dict with tokens as keys and list of dipendencies (from ROOT to token) as values\n",
        "    * Implementation:\n",
        "      *   Process the sentence\n",
        "      *   For each token extracts its dependency path\n",
        "\n",
        "*   dependency_path(token)\n",
        "    * Input: a token from Doc spaCy object\n",
        "    * Output: the dependency path from ROOT to the token as a list\n",
        "    * Implementation: \n",
        "      * starting from the dependency of the token itself, retrace the dependency tree until finding the ROOT (token.head == token)\n",
        "      * return the reverse of the dependency list in order to output ROOT -> token path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6gj-A6ZFhmW"
      },
      "source": [
        "def path_to_root(sentence):\n",
        "  doc = process_sentence(sentence)\n",
        "  d = {}\n",
        "  for token in doc:\n",
        "    d[token] = dependency_path(token)\n",
        "  return d\n",
        "\n",
        "def dependency_path(token):\n",
        "  path = [token.dep_] # save the first dependency (the one of the token)\n",
        "  exit = False\n",
        "  # until the root has not been found\n",
        "  while(not exit):\n",
        "    # if this token is the root then exit\n",
        "    if(token.head == token):\n",
        "      exit = True\n",
        "    else:\n",
        "      # otherwise append the dependency of the head \n",
        "      path.append(token.head.dep_)\n",
        "      # start again from the head of the token\n",
        "      token = token.head\n",
        "  return list(reversed(path))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APZmv38YPd8z"
      },
      "source": [
        "# 2) Extract subtree of the dependents given a token\n",
        "\n",
        "*   get_subtree(token)\n",
        "    * Input: a Token object from spaCy\n",
        "    * Output: the list of the depentents from the token (its subtree)\n",
        "    * Implementation: creation of a list containing the tokens inside its subtree\n",
        "\n",
        "*   get_subtrees(sentence)\n",
        "    * Input: the sentence\n",
        "    * Output: dict\n",
        "      * keys: the tokens of the list\n",
        "      * values: for each token the list of its dependents (the subtree of the token)\n",
        "    * Implementation:\n",
        "      *   Process the sentence\n",
        "      *   For each token extracts its subtree using get_subtree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f1KpOJfpyYM"
      },
      "source": [
        "def get_subtree(token):\n",
        "  return [dipendent for dipendent in token.subtree]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGcjE129Pbd5"
      },
      "source": [
        "def get_subtrees(sentence):\n",
        "  doc = process_sentence(sentence)\n",
        "  d = {}\n",
        "  for token in doc:\n",
        "    d[token] = get_subtree(token)\n",
        "  return d "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_8UlEbBTMIA"
      },
      "source": [
        "# 3) Check if a given list of tokens (segment of a sentence) forms a subtree\n",
        "*   check_subtree(sentence, subsequence)\n",
        "    * Input: The sentence to parse and the ordered list of words to check\n",
        "    * Output: True if the list forms a subtree of the sentence, False otherwise\n",
        "    * Implementation:\n",
        "      * Extract every subtree of the sentence using get_subtrees\n",
        "      * Compare the input sequence with each subtree, if one matches the input sequence return True, False otherwise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zROqkubNQwMo"
      },
      "source": [
        "def check_subtree2(sentence, sequence):\n",
        "  subtrees = get_subtrees(sentence)\n",
        "  for token in subtrees:\n",
        "    subtree = subtrees[token]\n",
        "    if(len(subtree) == len(sequence)):\n",
        "      if([t.text for t in subtree] == sequence):\n",
        "        return True\n",
        "  return False"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPuwrcdOlOpW"
      },
      "source": [
        "# 4) Identify head of a span, given its tokens\n",
        "* head_span(span)\n",
        "  * Input: a string containing a span of words\n",
        "  * Output: the word which is the head of the span\n",
        "  * Implementation: \n",
        "    * parse the span to have a Doc object\n",
        "    * from the Doc object, take the Span of the entire sequence and return its root"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Htf0JplO1Z"
      },
      "source": [
        "def head_span(span):\n",
        "  doc = process_sentence(span)\n",
        "  return doc[:].root.text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbOR5MQDnSx5"
      },
      "source": [
        "# 5) Extract sentence subject, direct object and indirect object spans\n",
        "* extract(sentence)\n",
        "  * Input: the sentence\n",
        "  * Output: a dict with keys:\n",
        "    * ``Subject``\n",
        "    * ``Direct object`` \n",
        "    * ``Indirect object``  \n",
        "  and the corresponding list of words as values, for the subject it returns a list for each subtree depending on the subject\n",
        "\n",
        "  * Implementation: \n",
        "    * For each token of the Doc object check if the token is:\n",
        "      * ``nsubj`` which stand for Nominal Subject\n",
        "      * ``nsubjpass`` which stand for Nominal Subject Passive\n",
        "      * ``csubj`` which stand for Clausal Subject\n",
        "      * ``csubjpass`` which stand for Clausal Subject Passive\n",
        "      * ``expl`` which stand for Expletive Subject\n",
        "      * ``dobj`` which stand for Direct Object\n",
        "      * ``dative`` which stand for Indirect Object\n",
        "    * For each of these tokens save its subtree\n",
        "    * For each different type of subject of the sentence a list is created with each subtree, these lists are then returned in the final dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN7ZGBvgsvJY"
      },
      "source": [
        "def extract(sentence):\n",
        "  doc = process_sentence(sentence)\n",
        "  d = {}\n",
        "  d[\"nsubj\"], d[\"nsubjpass\"], d[\"dobj\"], d[\"dative\"], d[\"csubj\"], d[\"csubjpass\"], d[\"expl\"] = [], [], [], [], [], [], []\n",
        "  for token in doc:\n",
        "    if((token.dep_ == \"nsubj\") or\n",
        "       (token.dep_ == \"nsubjpass\") or\n",
        "       (token.dep_ == \"csubj\") or\n",
        "       (token.dep_ == \"csubjpass\") or\n",
        "       (token.dep_ == \"expl\") or\n",
        "       (token.dep_ == \"dobj\") or\n",
        "       (token.dep_ == \"dative\")):\n",
        "      for t in token.subtree:\n",
        "        d[token.dep_].append(t.text)\n",
        "  subject = []\n",
        "  for key in d:\n",
        "    if(key in [\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"expl\"]):\n",
        "      if(len(d[key]) != 0):\n",
        "        subject.append(d[key])\n",
        "\n",
        "  return {\"Subject\": subject if len(subject)!=0 else None,\n",
        "          \"Direct object\": d[\"dobj\"] if len(d[\"dobj\"])!=0 else None,\n",
        "          \"Indirect object\": d[\"dative\"] if len(d[\"dative\"])!=0 else None}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6QB-FPVswB7"
      },
      "source": [
        "# Execution\n",
        "Script which calls and executes the previous functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7XdoSXvx6xJ"
      },
      "source": [
        "# list of sentences used to test the functions and the underlying ideas:\n",
        "\n",
        "# sentence = \"Luca has been killed by a car\"\n",
        "# sentence = \"What she said is interesting\"\n",
        "# sentence = \"That his theory was flawed soon became obvious\"\n",
        "# sentence = \"What I need is a long holiday\"\n",
        "# sentence = \"To become an opera singer takes years of training\"\n",
        "# sentence = \"Being the chairman is a huge responsibility\"\n",
        "# sentence = \"There is a fly in my soup\"\n",
        "# sentence = \"That Fred is a funny comedian\"\n",
        "# sentence = \"There is a woman in the bus who is called Diana\"\n",
        "# sentence = \"There is a toy airplane on the grass in the backyard.\"\n",
        "# sentence = \"There is a red house over iyonder\"\n",
        "# sentence = \"What she said makes sense\"\n",
        "# sentence = \"He gave me a nice gift for Christmas.\"\n",
        "# sentence = \"I saw the man\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reANIlTKDVcW",
        "outputId": "9e81d42b-0d20-41cc-99da-84598d3f91f0"
      },
      "source": [
        "sentence = \"I watched a movie with Sisko.\"\n",
        "\n",
        "print(f\"Sentence: {sentence}\\n\")\n",
        "\n",
        "print(\"1) ------ ROOT to token -------\")\n",
        "d = path_to_root(sentence)\n",
        "for token in d:\n",
        "  print(f\"Token: {token} | path : {d[token]}\")\n",
        "\n",
        "print(\"\\n2) --------- Subtrees ---------\")\n",
        "subtrees = get_subtrees(sentence)\n",
        "for token in subtrees:\n",
        "  print(f\"Token: {token}\")\n",
        "  print(f\"--> Subtree: {subtrees[token]}\")\n",
        "\n",
        "print(\"\\n3) --------- Check subtree ---------\")\n",
        "seq = [\"I\"]\n",
        "seq1 = [\"a\", \"movie\", \"with\", \"Sisko\"]\n",
        "seq2 = [\"movie\", \"a\", \"with\", \"Sisko\"]\n",
        "print(f\"Sequence to check: {seq}\")\n",
        "print(f\"Is it a subtree? {check_subtree2(sentence, seq)}\")\n",
        "print(f\"Sequence to check: {seq1}\")\n",
        "print(f\"Is it a subtree? {check_subtree2(sentence, seq1)}\")\n",
        "print(f\"Sequence to check: {seq2}\")\n",
        "print(f\"Is it a subtree? {check_subtree2(sentence, seq2)}\")\n",
        "\n",
        "print(\"\\n4) --------- Head of span ----------\")\n",
        "span = \"I man world\"\n",
        "print(f\"Span: {span}\")\n",
        "print(f\"Head: {head_span(span)}\")\n",
        "\n",
        "print(\"\\n5) --------- Subject, Object, Indirect object ----------\")\n",
        "extracted = extract(sentence)\n",
        "for value in extracted:\n",
        "  print(value, extracted[value])\n",
        "\n",
        "sentence = \"Luca has been killed by a car.\"\n",
        "print(f\"\\nExample with: {sentence}\")\n",
        "print(\"In this case 'Luca' is a Nominal Subject Passive\")\n",
        "extracted = extract(sentence)\n",
        "for value in extracted:\n",
        "  print(value, extracted[value])\n",
        "\n",
        "sentence = \"There is a woman in the bus who is called Diana.\"\n",
        "print(f\"\\nExample with: {sentence}\")\n",
        "print(\"In this case 'who' is a Nominal Subject Passive and 'There' is a Expletive Subject\")\n",
        "extracted = extract(sentence)\n",
        "for value in extracted:\n",
        "  print(value, extracted[value])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: I watched a movie with Sisko.\n",
            "\n",
            "1) ------ ROOT to token -------\n",
            "Token: I | path : ['ROOT', 'nsubj']\n",
            "Token: watched | path : ['ROOT']\n",
            "Token: a | path : ['ROOT', 'dobj', 'det']\n",
            "Token: movie | path : ['ROOT', 'dobj']\n",
            "Token: with | path : ['ROOT', 'dobj', 'prep']\n",
            "Token: Sisko | path : ['ROOT', 'dobj', 'prep', 'pobj']\n",
            "Token: . | path : ['ROOT', 'punct']\n",
            "\n",
            "2) --------- Subtrees ---------\n",
            "Token: I\n",
            "--> Subtree: [I]\n",
            "Token: watched\n",
            "--> Subtree: [I, watched, a, movie, with, Sisko, .]\n",
            "Token: a\n",
            "--> Subtree: [a]\n",
            "Token: movie\n",
            "--> Subtree: [a, movie, with, Sisko]\n",
            "Token: with\n",
            "--> Subtree: [with, Sisko]\n",
            "Token: Sisko\n",
            "--> Subtree: [Sisko]\n",
            "Token: .\n",
            "--> Subtree: [.]\n",
            "\n",
            "3) --------- Check subtree ---------\n",
            "Sequence to check: ['I']\n",
            "Is it a subtree? True\n",
            "Sequence to check: ['a', 'movie', 'with', 'Sisko']\n",
            "Is it a subtree? True\n",
            "Sequence to check: ['movie', 'a', 'with', 'Sisko']\n",
            "Is it a subtree? False\n",
            "\n",
            "4) --------- Head of span ----------\n",
            "Span: I man world\n",
            "Head: man\n",
            "\n",
            "5) --------- Subject, Object, Indirect object ----------\n",
            "Subject [['I']]\n",
            "Direct object ['a', 'movie', 'with', 'Sisko']\n",
            "Indirect object None\n",
            "\n",
            "Example with: Luca has been killed by a car.\n",
            "In this case 'Luca' is a Nominal Subject Passive\n",
            "Subject [['Luca']]\n",
            "Direct object None\n",
            "Indirect object None\n",
            "\n",
            "Example with: There is a woman in the bus who is called Diana.\n",
            "In this case 'who' is a Nominal Subject Passive and 'There' is a Expletive Subject\n",
            "Subject [['who'], ['There']]\n",
            "Direct object None\n",
            "Indirect object None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1QsdmfJMUhp"
      },
      "source": [
        "# Optional part\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCfQKaGQMapF"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUzjjTlYI4zz",
        "outputId": "72d61195-fdb6-4106-f134-b4b1c990fbcf"
      },
      "source": [
        "import nltk\n",
        "from nltk.parse.transitionparser import TransitionParser\n",
        "from nltk.parse.transitionparser import Configuration\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "nltk.download('dependency_treebank')\n",
        "from nltk.parse import DependencyEvaluator\n",
        "import tempfile\n",
        "from nltk.corpus import dependency_treebank\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "import pickle\n",
        "from nltk.parse import DependencyEvaluator\n",
        "from os import remove\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package dependency_treebank to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ijnuqz9Mffp"
      },
      "source": [
        "TransitionParser_MLP(TransitionParser):  \n",
        "I extend the TransitionParser from nltk.parse.transitionparser to change the model used during trainining and predicting.  \n",
        "I choose a MLP classifier from sklearn and since the neural netwroks have a fluctuation on the accuracy, I take the avearge accuracy of 10 runs.  \n",
        "Then I test the new model against the default one (SVM) from the default TransitionParser"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D649kk7U2XXy"
      },
      "source": [
        "class TransitionParser_MLP(TransitionParser):\n",
        "\n",
        "  def __init__(self, alg_option):\n",
        "    TransitionParser.__init__(self, alg_option)\n",
        "\n",
        "  def train(self, depgraphs, modelfile, verbose=True):\n",
        "      try:\n",
        "          input_file = tempfile.NamedTemporaryFile(\n",
        "              prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n",
        "          )\n",
        "\n",
        "          if self._algorithm == self.ARC_STANDARD:\n",
        "              self._create_training_examples_arc_std(depgraphs, input_file)\n",
        "          else:\n",
        "              self._create_training_examples_arc_eager(depgraphs, input_file)\n",
        "\n",
        "          input_file.close()\n",
        "\n",
        "          x_train, y_train = load_svmlight_file(input_file.name)\n",
        "\n",
        "          model = MLPClassifier()\n",
        "\n",
        "          model.fit(x_train, y_train)\n",
        "          # Save the model to file name (as pickle)\n",
        "          pickle.dump(model, open(modelfile, \"wb\"))\n",
        "      finally:\n",
        "          remove(input_file.name)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdPYcC-VKd9c",
        "outputId": "4ac8c94c-2d1b-4ade-e753-bc88f06adeb5"
      },
      "source": [
        "# compare the performances\n",
        "# Since we don't have dependency labels (as in the lab) I take in account just the uas (unlabeled)\n",
        "\n",
        "times = 10\n",
        "\n",
        "p = 0\n",
        "for i in range(times):\n",
        "  tp = TransitionParser_MLP('arc-standard')\n",
        "  tp.train(dependency_treebank.parsed_sents()[:200], 'tp.model')\n",
        "  parses = tp.parse(dependency_treebank.parsed_sents()[-20:], 'tp.model')\n",
        "  de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-20:])\n",
        "\n",
        "  _, uas = de.eval()\n",
        "  p += uas\n",
        "\n",
        "print(f\"[arc-standard] Average accuracy of MLP: {p/times}\")\n",
        "\n",
        "tp = TransitionParser('arc-standard')\n",
        "tp.train(dependency_treebank.parsed_sents()[:200], 'tp.model')\n",
        "parses = tp.parse(dependency_treebank.parsed_sents()[-20:], 'tp.model')\n",
        "\n",
        "de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-20:])\n",
        "_, uas = de.eval()\n",
        "\n",
        "print(f\"[arc-standard] Accuracy of the default model (SVC): {uas}\")\n",
        "\n",
        "p = 0\n",
        "for i in range(times):\n",
        "  tp = TransitionParser_MLP('arc-eager')\n",
        "  tp.train(dependency_treebank.parsed_sents()[:200], 'tp.model')\n",
        "  parses = tp.parse(dependency_treebank.parsed_sents()[-20:], 'tp.model')\n",
        "\n",
        "  de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-20:])\n",
        "  _, uas = de.eval()\n",
        "  p += uas\n",
        "\n",
        "print(f\"[arc-eager] Average accuracy of MLP: {p/times}\")\n",
        "\n",
        "tp = TransitionParser('arc-eager')\n",
        "tp.train(dependency_treebank.parsed_sents()[:200], 'tp.model')\n",
        "parses = tp.parse(dependency_treebank.parsed_sents()[-20:], 'tp.model')\n",
        "\n",
        "de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-20:])\n",
        "_, uas = de.eval()\n",
        "\n",
        "print(f\"[arc-eager] Accuracy of the default model (SVC): {uas}\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            "[arc-standard] Average accuracy of MLP: 0.6996039603960394\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            "[LibSVM][arc-standard] Accuracy of the default model (SVC): 0.8198019801980198\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            "[arc-eager] Average accuracy of MLP: 0.7122772277227721\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            "[LibSVM][arc-eager] Accuracy of the default model (SVC): 0.8237623762376237\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quDddAhYPpcv"
      },
      "source": [
        "Here I try the random forest as other model with the results which are below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDcP-BSiJiFJ"
      },
      "source": [
        "class TransitionParser_RF(TransitionParser):\n",
        "\n",
        "  def __init__(self, alg_option):\n",
        "    TransitionParser.__init__(self, alg_option)\n",
        "\n",
        "  def train(self, depgraphs, modelfile, verbose=True):\n",
        "      try:\n",
        "          input_file = tempfile.NamedTemporaryFile(\n",
        "              prefix=\"transition_parse.train\", dir=tempfile.gettempdir(), delete=False\n",
        "          )\n",
        "\n",
        "          if self._algorithm == self.ARC_STANDARD:\n",
        "              self._create_training_examples_arc_std(depgraphs, input_file)\n",
        "          else:\n",
        "              self._create_training_examples_arc_eager(depgraphs, input_file)\n",
        "\n",
        "          input_file.close()\n",
        "\n",
        "          x_train, y_train = load_svmlight_file(input_file.name)\n",
        "\n",
        "          model = clf = RandomForestClassifier()\n",
        "\n",
        "          model.fit(x_train, y_train)\n",
        "          # Save the model to file name (as pickle)\n",
        "          pickle.dump(model, open(modelfile, \"wb\"))\n",
        "      finally:\n",
        "          remove(input_file.name)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW95sccMJo8K",
        "outputId": "ab29a30e-df95-4b3a-fef3-d2c16e92d8b7"
      },
      "source": [
        "times = 10\n",
        "\n",
        "p = 0\n",
        "for i in range(times):\n",
        "  tp = TransitionParser_RF('arc-standard')\n",
        "  tp.train(dependency_treebank.parsed_sents()[:200], 'tp.model')\n",
        "  parses = tp.parse(dependency_treebank.parsed_sents()[-20:], 'tp.model')\n",
        "  de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-20:])\n",
        "\n",
        "  _, uas = de.eval()\n",
        "  p += uas\n",
        "\n",
        "print(f\"[arc-standard] Average accuracy of the random forests: {p/times}\")\n",
        "\n",
        "p = 0\n",
        "for i in range(times):\n",
        "  tp = TransitionParser_RF('arc-eager')\n",
        "  tp.train(dependency_treebank.parsed_sents()[:200], 'tp.model')\n",
        "  parses = tp.parse(dependency_treebank.parsed_sents()[-20:], 'tp.model')\n",
        "\n",
        "  de = DependencyEvaluator(parses, dependency_treebank.parsed_sents()[-20:])\n",
        "  _, uas = de.eval()\n",
        "  p += uas\n",
        "\n",
        "print(f\"[arc-eager] Average accuracy of the random forests: {p/times}\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            "[arc-standard] Average accuracy of the random forests: 0.7766336633663365\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            " Number of training examples : 200\n",
            " Number of valid (projective) examples : 200\n",
            "[arc-eager] Average accuracy of the random forests: 0.7827722772277227\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}